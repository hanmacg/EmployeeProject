---
output:
  md_document:
    variant: markdown_github
---

# Purpose

This is my data cleaning for my machine learning project on employee attrition.


```{r}

rm(list = ls()) # Clean your environment:
gc() # garbage collection - It can be useful to call gc after a large object has been removed, as this may prompt R to return memory to the operating system.
library(tidyverse)
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))
```

First I read in the data
```{r}
employeedat <- read.csv("C:/Users/hanna/OneDrive/Documents/Economics Masters/Data Science/Machine learning/data/Employee.csv")
head(employeedat)
tab <- employeedat %>% select(Education, Age, Gender, LeaveOrNot)
as_tibble(tab)
```

Next I do some cleaning as many of the variables are characters/categorical

```{r}
#create a binary variable for gender - female is 0 and male is 1
employeedat <- employeedat %>%
  mutate(gender_numeric = ifelse(Gender == "Female", 0, 1))
#create a binary variable for ever-benched - i.e., they kept out of projects for a month or more
employeedat <- employeedat %>%
  mutate(benched_numeric = ifelse(EverBenched == "No", 0, 1))
#create a variable for education category where Bachelors =0, Master's =1 and PhD = 2
employeedat <- employeedat %>%
  mutate(education_numeric = case_when(
    Education == "Bachelors" ~ 0,
    Education == "Masters" ~ 1,
    Education == "PHD" ~ 2
  ))

#How many PHD individuals?
phd_count <- sum(employeedat$Education == "PHD")

#create binary variables for whether a person is from Bangalore, Pune or New Delhi
employeedat <- employeedat %>%
  mutate(bangalore = ifelse(City == "Bangalore", 1, 0)) %>% 
    mutate(pune = ifelse(City == "Pune", 1, 0)) %>% 
    mutate(new_delhi = ifelse(City == "New Delhi", 1, 0))


```

Next, I conduct some exploratory data analysis to see the relationships between my target and features of the dataset. I visualise my data using ggplot.

```{r}
pacman::p_load(forcats)

#graphing city spread among employees
employeedat %>% 
mutate(City2 = forcats::fct_infreq(City)) %>% 
ggplot() + 
geom_bar(aes(x = City2), fill = "steelblue", alpha = 0.7) + 
    
coord_flip() + 
labs(x = "Count", y = "City", title = "City spread among employees", 
    caption = "Data from Kaggle") 


#graph spread of education
employeedat %>% 
mutate(Education2 = forcats::fct_infreq(Education)) %>% 
ggplot() + 
geom_bar(aes(x = Education2), fill = "steelblue", alpha = 0.7) + 
    
coord_flip() + 
labs(x = "Count", y = "Education", title = "Education spread among employees", 
    caption = "Data from Kaggle") 

##Looking at employee attrition against age. It seems that attrition is higher among younger individuals and lower for older individuals.

ggplot(employeedat, aes(x = Age, y = LeaveOrNot)) +
  geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(x = "Age", y = "Employee Attrition") +
  theme_minimal()

#Plotting attrition rates by education
ggplot(employeedat, aes(x = Education, fill = factor(LeaveOrNot))) +
  geom_bar(position = "fill") +
  labs(x = "Education", y = "Attrition Rate", fill = "Attrition") +
  ggtitle("Attrition Rates by Education")

#Plotting attrition rates by joining year
ggplot(employeedat, aes(x = factor(JoiningYear), fill = factor(LeaveOrNot))) +
  geom_bar(position = "fill") +
  labs(x = "Joining Year", y = "Attrition Rate", fill = "Attrition") +
  ggtitle("Attrition Rates by Joining Year") + scale_fill_manual(values = c("lavender", "grey"))+ theme_minimal()


#scatter plot of age and experience
ggplot(employeedat, aes(x = Age, y = ExperienceInCurrentDomain, color = factor(LeaveOrNot))) +
  geom_point() +
  labs(x = "Age", y = "Experience", color = "Attrition") +
  ggtitle("Relationship between Age, Experience, and Attrition")


ggplot(employeedat, aes(x = Age)) +
  geom_histogram(binwidth = 2, fill = "lightblue", color = "black") +
  labs(x = "Age", y = "Count") +
  ggtitle("Distribution of Age") + theme_minimal()


ggplot(employee_data, aes(x = Experience)) +
  geom_histogram(binwidth = 1, fill = "lightgreen", color = "black") +
  labs(x = "Experience", y = "Count") +
  ggtitle("Distribution of Experience")

# g <-employeedat %>% 
# ggplot() + 
# geom_line(aes(x = ExperienceInCurrentDomain, y = Age, color = LeaveOrNot), alpha = 0.8, 
#     size = 1)
# g
# 
# # Notice that I used alpha to adjust the line opacity (useful
# # when having overlapping lines e.g.)
# 
# print(g)

#Table of attrition rate
attrition_rate_education <- employeedat %>%
  group_by(Education) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

# Calculate attrition rate by Gender
attrition_rate_gender <- employeedat %>%
  group_by(Gender) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by City
attrition_rate_city <- employeedat %>%
  group_by(City) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by Joining Year
attrition_rate_year <- employeedat %>%
  group_by(JoiningYear) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by Payment Tier
attrition_rate_pay <- employeedat %>%
  group_by(PaymentTier) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

# Print attrition rate tables
print(attrition_rate_education)
print(attrition_rate_gender)
print(attrition_rate_city)
print(attrition_rate_year)
print(attrition_rate_pay)

# # Define category labels
# education_labels <- c("Bachelors", "Masters", "PHD")  # Update with your education categories
# gender_labels <- c("Female", "Male")  # Update with your gender categories
# city_labels <- c("Bangalore", "New Delhi", "Pune")  # Update with your city categories
# year_labels <- c("2012", "2013", "2014", "2015", "2016", "2017", "2018")  # Update with your year categories
# pay_labels <- c("1", "2", "3")  # Update with your pay categories
# 
# # Reshape the attrition rate tables
# attrition_table <- bind_rows(
#   Education = attrition_rate_education,
#   Gender = attrition_rate_gender,
#   City = attrition_rate_city,
#   Year = attrition_rate_year,
#   Pay = attrition_rate_pay
# ) %>%
#   rownames_to_column(var = "Category") %>%
#   mutate(Category = case_when(
#     Category == "Education" ~ education_labels,
#     Category == "Gender" ~ gender_labels,
#     Category == "City" ~ city_labels,
#     Category == "Year" ~ year_labels,
#     Category == "Pay" ~ pay_labels,
#     TRUE ~ NA_character_
#   )) %>%
#   select(Category, Attrition_Rate)
# 
# kable(attrition_table, caption = "Attrition Rate Table", align = "c") %>%
#   kable_styling()


##attrition rate by education
attrition_table_e <- attrition_rate_education
kable(attrition_table_e, caption = "Attrition Rate Table", align = "c") %>%
  kable_styling()

#attrition rate by gender
attrition_table_g <- attrition_rate_gender
kable(attrition_table_g, caption = "Attrition Rate Table", align = "c") %>%
  kable_styling()


combined_table <- bind_rows(
  Education = attrition_table_e,
  Gender = attrition_table_g
)
kable(combined_table, caption = "Attrition Rate Table", align = "c") %>%
  kable_styling()


########################
# Term_refs_perc <- function(attrition_rate_education, Terms = c("Bachelors", "Masters", "PHD" )){
# 
#     Term_ref_freq <-
#         attrition_rate_education %>%
#         group_by(Education) 
# 
#     Term_ref_freq
# 
# }
# 
# Term_refs_perc2 <- function(attrition_rate_gender, Terms = c("Female", "Male" )){
# 
#     Term_ref_freq <-
#         attrition_rate_gender %>%
#         group_by(Gender) 
# 
#     Term_ref_freq
# 
# }
# 
# Result <- bind_rows(
#         Term_refs_perc(attrition_rate_education, Terms = c("Bachelors", "Masters", "PHD")),
#         Term_refs_perc2(attrition_rate_gender, Terms = c("Female", "Male"))
#     ) 
# 
# Result

```

Split the data into a test and training set. I use simple random sampling methods to make a 70-30 split between my training and testing data. 

```{r}
set.seed(123)  # Set the seed for reproducibility
split_1  <- initial_split(employeedat, prop = 0.7)  # Split the dataset 
train  <- training(split_1)  # Training set
test   <- testing(split_1)  # Test set

#I then look at some resampling methods. Validation based approaches split the training set even further to create a new training and validation set. Resampling methods repeatedly fit out model to different parts of the training data and test against different validation sets.  
#firstly the bootstrap method. Make sure to have the rsample package downloaded. 
bootstraps(employeedat, times = 10)

#next is the k-fold cross validation 
vfold_cv(employeedat, v = 10)

cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

knn_fit <- train(
  LeaveOrNot ~ ., 
  data = train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)

knn_fit

# the above chose k =11 

ggplot(knn_fit)


```

It is also important to consider the bias-variance trade-off. Prediction errors are generally a result of either bias or variance. In general, decreasing bias will almost always lead to greater variance. Bias is the difference between the expected prediction and the correct value (linear models have high levels of bias).

Variance error is the variability of a model prediction for a given data point. Some models can potentially overfit the training data, resulting in accurate results against the training data but typically poor results against the test data. 

We can adjust the model hyperparameters to achieve the best mix of bias and variance. 


Target engineering:

Since my predictor variable (Leave or Not) is binary, there is no need for target engineering.

Feature Engineering:
Most of the features are categorical. 


Linear regression and MLR:
```{r}
model1 <- lm(LeaveOrNot ~ education_numeric, employeedat)
model2 <- lm(LeaveOrNot ~ education_numeric + gender_numeric, employeedat)

set.seed(123)  
(cv_model1 <- train(form = LeaveOrNot ~ education_numeric, 
  data = train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

set.seed(123)  
(cv_model2 <- train(form = LeaveOrNot ~ education_numeric + gender_numeric, 
  data = train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))

resamp <- resamples(list(model1 = cv_model1, model2 = cv_model2))
summary_resamp <- summary(resamp)
print(summary_resamp)

#using glmnet to conduct regularised regression analysis - however, this is better suited to continuous or count outcome variables.

X <- model.matrix(LeaveOrNot ~ ., train)[, -1]
Y <- train$LeaveOrNot

ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0)
plot(ridge)

lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1)

plot(lasso)

cv_glmnet$bestTune

```
**remember to talk about the loss function

Decision trees

For decision trees, we use annova instead of lm.
```{r}
pacman::p_load(dplyr, ggplot2, rpart, caret, rpart.plot, 
               vip, pdp, doParallel, foreach, 
               ipred, ranger, gbm, xgboost, AmesHousing)
pacman::p_load(rsample)

split  <- rsample::initial_split(employeedat, prop = 0.7, 
                                 strata = "LeaveOrNot")
emp_train  <- rsample::training(split)
emp_dt1 <- rpart(
  formula = LeaveOrNot ~ .,
  data    = emp_train,
  method  = "class"
)
emp_dt1
plot(emp_dt1)

emp_test <- rsample::testing(split)
predictions

actual_labels <- emp_test$LeaveOrNot

# Calculate the accuracy
accuracy <- sum(predictions == actual_labels) / length(actual_labels) * 100

# Print the accuracy
cat("Accuracy:", accuracy, "%\n")

##pruning - before pruning, variance is high and bias is low

cv_emp_dt1 <- prune(emp_dt1, cp = emp_dt1$cptable[which.min(emp_dt1$cptable[,"xerror"]),"CP"])

plotcp(emp_dt1)

plot(cv_emp_dt1)
text(cv_emp_dt1)

cv_emp_dt1



```


Bagging - bootstrap aggregating. This is where multiple bootstrap copies of the original training data are created. For classification, predictions combined by averaging estimated class. For regression, new predictions are made by averaging predictions. Works well for high-variance algorithms such as decision trees of KNN.

```{r}
library(randomForest)
# train bagged model
set.seed(123)

# Train bagged model
emp_bag1 <- randomForest(
  formula = LeaveOrNot ~ .,
  data = emp_train,
  ntree = 100,  # Number of trees
  mtry = ncol(emp_train) - 1,  # Number of predictors sampled for each tree
  replace = TRUE,  # Sampling with replacement
  importance = TRUE  # Compute variable importance
)

emp_bag1

```

The issue with bagging is that the trees in bagging are not independent (tree correlation). Random Forests help to reduce tree correlation. It does so by using split-variable randomisation. Number of trees should be 10 times number of feautures, as a rule of thumb. 

```{r}
library(ranger)
# number of features
n_features <- length(setdiff(names(emp_train), "LeaveOrNot"))

emp_train$LeaveOrNot <- as.factor(emp_train$LeaveOrNot)

emp_rf1 <- ranger(
  LeaveOrNot ~ ., 
  data = emp_train,
  mtry = floor(sqrt(n_features)),
  respect.unordered.factors = "order",
  seed = 123)

(default_rmse <- sqrt(emp_rf1$prediction.error))

#prediction
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
predictions <- predict(emp_rf1, data = emp_test)
predictions
predicted_labels <- predictions$predictions
predicted_labels

#accuracy
actual_labels <- emp_test$LeaveOrNot
accuracy <- sum(predictions$predictions == actual_labels) / length(actual_labels)
accuracy

#precision
precision <- sum(predicted_labels == 1 & actual_labels == 1) / sum(predicted_labels == 1)
precision

#recall
recall <- sum(predicted_labels == 1 & actual_labels == 1) / sum(actual_labels == 1)
recall

#f1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score


library(pROC)
predicted_probs <- as.numeric(predictions$predictions)

# Convert the actual labels to a numeric format (0 and 1)
numeric_labels <- as.numeric(actual_labels) - 1

# Calculate the ROC curve and AUC-ROC
roc_obj <- roc(numeric_labels, predicted_probs)
auc_roc <- auc(roc_obj)
auc_roc

```

There are several hyperparamters to consider in this model: he number of trees, the number of features to consider at a given split, the complexity of each tree, the sampling scheme, the splitting rule to use during tree construction.

Node size is the most common method to control tree complexity.

The first parameter I am going to adjust is the number of trees. If I have 15 variables, I will make 150 trees. The defualt above was 500 trees. 
```{r}
n_features <- length(setdiff(names(emp_train), "LeaveOrNot"))

emp_train$LeaveOrNot <- as.factor(emp_train$LeaveOrNot)

emp_rf1 <- ranger(
  LeaveOrNot ~ ., 
  data = emp_train,
  mtry = floor(sqrt(n_features)),
  num.trees = 150, 
  respect.unordered.factors = "order",
  seed = 123)

(default_rmse <- sqrt(emp_rf1$prediction.error))
```

We could alter the m(try) parameter, but currently it is set to the square root of the number of parameters, given that this is a classification problem.

I next look at adjusting the tree complexity. The hyperparameter that I use is node size. The default used above was a node size of 1. Small values for node sieze result in more complex trees which potentially capture more patterns in the data but may lead to overfitting. Increasing the node size leads to simpler trees that are less prone to overfitting but may overlook finer patterns in the data.

```{r}
emp_node5 <- ranger(
  LeaveOrNot ~ ., 
  data = emp_train,
  mtry = floor(sqrt(n_features)),
  num.trees = 150,
  respect.unordered.factors = "order",
  seed = 123,
  min.node.size = 5  # Adjust the node size here
)

(default_rmse <- sqrt(emp_node5$prediction.error))
```

Next, it is worth examining sample size and whether to sample with or without replacement. In datasets with many categorical variables, such as this one, sampling with replacement can lead to biased variable split selection. 

```{r}
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = LeaveOrNot ~ ., 
  data = emp_train, 
  num.trees = 150,
  mtry = floor(sqrt(n_features)),
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = LeaveOrNot ~ ., 
  data = emp_train, 
  num.trees = 150,
  mtry = floor(sqrt(n_features)),
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
p1 <- vip::vip(rf_impurity, num_features = 14, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 14, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)

```
From the above plots, we can see that the joining year is the most important determinant for employee attrition.

The next possible model is a GSM (gradient boosting models). These build an ensemble of shallow trees in sequence with each tree learning and improving on the previous one. Models are added sequentially. It attacks the variance-bias tradeoff by starting with a weak model.

Gradient boosting is a gradient descent algorithm.

In a simple GBM, there are two catgeroeis of hyperparameters (boosting and tree-specific). Under boosting, the two main hyperparameters are number of trees and learning rate (shrinkage). Two main tree parameters are tree depth and minimum number of observations in terminal nodes.
```{r}
library(gbm)
set.seed(123)  # for reproducibility
emp_gbm1 <- gbm(
  formula = LeaveOrNot ~ .,
  data = emp_train,
  distribution = "gaussian",  # SSE loss function
  n.trees = 5000,
  shrinkage = 0.1,
  interaction.depth = 3,
  n.minobsinnode = 10,
  cv.folds = 10
)
#check errors and data for this
```

Lastly, XGBoost model - this requires us to encode all categorical variables numerically. it provides traditional boosting and tree-based hyperparameters. Xgboost also provides additional hyperparameters that can help reduce the chances of overfitting, leading to imporved accuracy.
