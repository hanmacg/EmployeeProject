---
# IMPORTANT: Change settings here, but DO NOT change the spacing.
# Remove comments and add values where applicable.
# The descriptions below should be self-explanatory

title: "Predicting Employee Attrition"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# --------- Thesis title (Optional - set to FALSE by default).
# You can move the details below around as you please.
Thesis_FP: FALSE
# Entry1: "An unbelievable study with a title spanning multiple lines."
# Entry2: "\\textbf{Some Guy}" # textbf for bold
# Entry3: "A thesis submitted toward the degree of Doctor of Philosophy"
# Uni_Logo: Tex/Logo.png # Place a logo in the indicated location (from your root, e.g. defaults to ~/Tex/Logo.png) and uncomment this line. Leave uncommented for no image
# Logo_width: 0.3 # If using a logo - use this to set width (size) of image
# Entry4: "Under the supervision of: \\vfill Prof. Joe Smith and Dr. Frank Smith"
# Entry5: "Stellenbosch University"
# Entry6: April 2020
# Entry7:
# Entry8:

# --------- Front Page
# Comment: ----- Follow this pattern for up to 5 authors
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Hannah MacGinty"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University, Cape Town, South Africa" # First Author's Affiliation
Email1: "21082022\\@sun.ac.za" # First Author's Email address

# Author2: "John Smith"
# Ref2: "Some other Institution, Cape Town, South Africa"
# Email2: "John\\@gmail.com"
CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

# Author3: "John Doe"
# Email3: "Joe\\@gmail.com"

CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

# Comment out below to remove both. JEL Codes only given if keywords also given.
# keywords: "Multivariate GARCH \\sep Kalman Filter \\sep Copula" # Use \\sep to separate
# JELCodes: "L250 \\sep L100"

# ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# --------- page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g.
# This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper.

# --------- Line numbers
linenumbers: FALSE # Used when submitting to journal

# ---------- References settings:
# You can download cls format here: https://www.zotero.org/ - simply search for your institution. You can also edit and save cls formats here: https://editor.citationstyles.org/about/
# Hit download, store it in Tex/ folder, and change reference below - easy.
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
# By default, the bibliography only displays the cited references. If you want to change this, you can comment out one of the following:
#nocite: '@*' # Add all items in bibliography, whether cited or not
# nocite: |  # add specific references that aren't cited
#  @grinold2000
#  @Someoneelse2010

# ---------- General:
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Labour Economics"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.

### Adding additional latex packages:
# header-includes:
#    - \usepackage{colortbl} # Add additional packages here.

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
abstract: |
  This paper investigates employee attributes and attrition. Random forests are used to build a model to predict employee attrition based on key attributes, such as education, pay, gender, and age, among others.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
Example_data <- Texevier::Ex_Dat

# Notice that as you are working in a .Rproj file (I am assuming you are) - the relative paths of your directories start at your specified root.
# This means that when working in a .Rproj file, you never need to use getwd() - it is assumed as your base root automatically.
write_rds(Example_data, path = "data/Example_data.rds")

```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}

Losing employees can be costly for businesses. Predicting attrition and its key determinants or attributes can help estimate future employee turnover and attempt to reduce it.

```{r}
employee <- read.csv("C:/Users/hanna/OneDrive/Documents/Data Science/Project/EmployeeProject/Final_Project/data/Employee.csv")
```

# Exploratory Data Analaysis {-}

The dataset contains information on 4653 employees and whether they attrited or not. The employee attributes available include demographic information such as age and gender. Employees are based in one of three major cities in India, namely Bangalore, Pune and New Delhi. The year an employee joined a company (Joining Year), ranging from 2014 to 2018, is also considered. 

Given that earnings are generally an important determinant in whether an employee leaves their job, their payment tier, scaled from 1, being the highest, and 3, being the lowest, is included in the data. Additionally, years of experience in their current field is included as well as their highest level of education (Bachelor's, Master's, PhD). There is also information on whether an employee kept out of projects for 1 month or more, which could potentially indicates an employee's lack of interest in work or plans to leave the company. 

```{r}
#spread of data across city and education
pacman::p_load(forcats)

library(wesanderson)
palette <- wes_palette("Zissou1", 2)
palette2 <- wes_palette("Zissou1", 1)

source("code/plot_spread.R")
spread <- plot_spread(employee, City)
spread
```


```{r}
spread_ed <- plot_spread(employee, Education)
spread_ed
```


Spreading the data among city, most of the employees are from Bangalore. Additionally, most employees have a Bachelors degree. Only 179 employees have a PHD and 873 have Master's degrees. Looking at age, which ranges between 22 and 41, the majority of employees are in their mid to late twenties, skewing the distribution to right.  

Regarding experience, very few individuals have 6 or 7 years experience (only 16 employees in total) in their current field of work, most likely due to the young employee base. Most commonly, individuals have 2 years experience. 

2017 saw the most employees join the company. There was a substantial fall in employees joining in 2018. Only 367 employees joined in 2018, compared to 1180 in 2017.

```{r}
#distribution
age_plot <- ggplot(employee, aes(x = Age)) +
  geom_histogram(binwidth = 2, fill = palette2, color = "black") +
  labs(x = "Age", y = "Count")  +
  ggtitle("Distribution of Age") + theme_classic()
age_plot
```


```{r}
exp_plot <- ggplot(employee, aes(x = ExperienceInCurrentDomain)) +
  geom_histogram(binwidth = 1, fill = palette2, color = "black") +
  labs(x = "Experience", y = "Count") +
  ggtitle("Distribution of Experience") +theme_classic()
exp_plot
```


```{r}
year_plot <- ggplot(employee, aes(x = JoiningYear)) +
  geom_histogram(binwidth = 1, fill = palette2, color = "black") +
  labs(x = "Year Joined", y = "Count") +
  ggtitle("Distribution of Joining Year") +theme_classic()
year_plot

```

Attrition rates are highest among those with Master's degrees. Nearly fifty percent of those with Master's degrees left their job. When looking across joining year, almost all the employees that joined in 2018 resigned. It is possible that some event occurred in 2018 that caused that cohort to leave within the next two years. 

```{r}
#Attrition Rate
educ_plot <- ggplot(employee, aes(x = Education, fill = factor(LeaveOrNot))) +
  geom_bar(position = "fill") + scale_fill_manual(values = palette) +
  labs(x = "Education", y = "Attrition Rate", fill = "Attrition") +
  ggtitle("Attrition Rates by Education") + theme_classic()
educ_plot
```


```{r}
year_att_plot <- ggplot(employee, aes(x = factor(JoiningYear), fill = factor(LeaveOrNot))) +
  geom_bar(position = "fill") +
  labs(x = "Joining Year", y = "Attrition Rate", fill = "Attrition") + scale_fill_manual(values = palette) +
  ggtitle("Attrition Rates by Joining Year") + theme_minimal()

year_att_plot
```

The table below presents the attrition rate across education, gender, city, joining year, experience, and pay level. 

```{r ShortTableds, results = 'asis'}

library(xtable)

#Table of attrition rate
attrition_rate_education <- employee %>%
  group_by(Education) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

# Calculate attrition rate by Gender
attrition_rate_gender <- employee %>%
  group_by(Gender) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by City
attrition_rate_city <- employee %>%
  group_by(City) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by Joining Year
attrition_rate_year <- employee %>%
  group_by(JoiningYear) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by Payment Tier
attrition_rate_pay <- employee %>%
  group_by(PaymentTier) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

education_labels <- c("Bachelors", "Masters", "PHD") 
city_labels <- c("Bangalore", "New Delhi", "Pune") 
gender_labels <- c("Female", "Male")  
year_labels <- c("2012", "2013", "2014", "2015", "2016", "2017", "2018")  
pay_labels <- c("1", "2", "3") 

education_table <- data.frame(Variable = education_labels, Attrition_Rate = attrition_rate_education$Attrition_Rate)
city_table <- data.frame(Variable = city_labels, Attrition_Rate = attrition_rate_city$Attrition_Rate)
gender_table <- data.frame(Variable =gender_labels, Attrition_Rate = attrition_rate_gender$Attrition_Rate)
year_table <- data.frame(Variable = year_labels, Attrition_Rate = attrition_rate_year$Attrition_Rate)
pay_table <- data.frame(Variable = pay_labels, Attrition_Rate = attrition_rate_pay$Attrition_Rate)

colnames(education_table) <- c("Variable", "Attrition Rate")
colnames(city_table) <- c("Variable", "Attrition Rate")
colnames(gender_table) <- c("Variable", "Attrition Rate")
colnames(year_table) <- c("Variable", "Attrition Rate")
colnames(pay_table) <- c("Variable", "Attrition Rate")

combined_table <- rbind(
  data.frame(Variable = "Education", Category = education_table$Variable, `Attrition Rate` = education_table$`Attrition Rate`),
  data.frame(Variable = "City", Category = city_table$Variable, `Attrition Rate` = city_table$`Attrition Rate`),
  data.frame(Variable = "Gender", Category = gender_table$Variable, `Attrition Rate` = gender_table$`Attrition Rate`),
  data.frame(Variable = "Year", Category = year_table$Variable, `Attrition Rate` = year_table$`Attrition Rate`),
  data.frame(Variable = "Pay", Category = pay_table$Variable, `Attrition Rate` = pay_table$`Attrition Rate`)
)

table_ds <- xtable(combined_table, caption = "Attrition Rate Across Categories \\label{tab1}")
  print.xtable(table_ds,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )

```


# Feature and Target Engineering {-}

Since my predictor variable (Leave or Not) is binary, there is no need for target engineering.

Regarding feature engineering, most of the features are categorical. Gender and whether a person benched or not (removed themselves from projects in the lst month) are transformed into dummies. Joining year is one-hot encoded, resulting in binary variables for each of the 5 joining years. 

Education is label encoded as it can be ordered (Bachelors being the lowest level of education, Master's one higher and PHD being the highest level of education). Payment Tier and experience in the current domain were already label encoded and thus do not require further engineering. 

Since age is numeric and random forests are able to handle both numeric and categorical variables, it is not altered.

```{r}
#create a binary variable for gender - female is 0 and male is 1
employee <- employee %>%
  mutate(male = ifelse(Gender == "Female", 0, 1))

#create a binary variable for ever-benched - i.e., they kept out of projects for a month or more
employee <- employee %>%
  mutate(benched = ifelse(EverBenched == "No", 0, 1))

#One hot encoding
employee <- employee %>%
  mutate(bangalore = ifelse(City == "Bangalore", 1, 0)) %>% 
    mutate(pune = ifelse(City == "Pune", 1, 0)) %>% 
    mutate(new_delhi = ifelse(City == "New Delhi", 1, 0))

#lable encode education category where Bachelors =1, Master's =2 and PhD = 3
employee <- employee %>%
  mutate(education = case_when(
    Education == "Bachelors" ~ 1,
    Education == "Masters" ~ 2,
    Education == "PHD" ~ 3
  ))

#one-hot encode joining year
data <- data.frame(JoiningYear = c("2014", "2015", "2016", "2017", "2018"))

employee <- employee %>%
  mutate(Y2014 = ifelse(JoiningYear == "2014", 1, 0)) %>% 
    mutate(Y2015 = ifelse(JoiningYear == "2015", 1, 0)) %>% 
    mutate(Y2016 = ifelse(JoiningYear == "2016", 1, 0)) %>% 
    mutate(Y2017 = ifelse(JoiningYear == "2017", 1, 0)) %>% 
     mutate(Y2018 = ifelse(JoiningYear == "2018", 1, 0))


```

# Logistic Regression {-}

Logistic regression is often used for binary classification problems. 

Using logistic regression, three models are estimated to assess the accuracy of predicting employee attrition. Model 1 regresses employee attrition on education level. Model 2 adds payment tier as another feature. Model 3 regresses employee attrition on all available features.

```{r}

library(caret)
library(vip)
library(rsample)
library(rpart)
library(rpart.plot)
library(broom)
library(vip)
library(kableExtra)

#getting rid of duplicates in dataset
employeedata <- subset(employee, select = c(LeaveOrNot, Age, male, benched, ExperienceInCurrentDomain, bangalore, pune, new_delhi, education, Y2014, Y2015, Y2016, Y2017, Y2018, PaymentTier))
# 

df <- employeedata %>% mutate_if(is.ordered, factor, ordered = FALSE)
df$LeaveOrNot <- factor(df$LeaveOrNot, levels = c(0, 1))
df$male <- factor(df$male, levels = c(0, 1))
df$benched <- factor(df$benched, levels = c(0, 1))
df$bangalore <- factor(df$bangalore, levels = c(0, 1))
df$pune <- factor(df$pune, levels = c(0, 1))
df$new_delhi <- factor(df$new_delhi, levels = c(0, 1))
# 
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = .7, strata = "LeaveOrNot")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)


#Let's make various models for comparison
set.seed(123)
cv_model1 <- train(
  LeaveOrNot ~ education,
  data = churn_train,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

#
set.seed(123)
cv_model2 <- train(
  LeaveOrNot ~ education + PaymentTier,
  data = churn_train,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)


set.seed(123)
cv_model3 <- train(
  LeaveOrNot ~ Age + male + benched + ExperienceInCurrentDomain + bangalore + pune + education + PaymentTier + Y2014 + Y2015 + Y2016 + Y2017,
  data = churn_train,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

s <- summary(
  resamples(
    list(
      model1 = cv_model1,
      model2 = cv_model2,
      model3 = cv_model3
    )
  )
)$statistics$Accuracy



```

```{r ShortTable, results = 'asis'}


data <- s %>% tibble::as_tibble()

table <- xtable(data, caption = "Accuracy across logistic models \\label{tab1}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )

```

From the logistic regression, model accuracy ranges from 70 and 76 percent for the full model. The weakest model is model 1, which regresses attrition on education, has an accuracy rate between 63 and 66 percent.

In terms of the most important features, gender and the joining year of 2017 are the most important predictive features. Most of the other variables do carry some level of importance, therefore the model is not overly reliant on gender and joining year.

```{r}
pred_class <- predict(cv_model3, churn_train)

#create confusion matrix
# confusionMatrix(
#   data = relevel(pred_class, ref = "1"),
#   reference = relevel(churn_train$LeaveOrNot, ref = "1")
# )

important_f <- vip(cv_model3, num_features = 14)
important_f

#calculate metrics from the confusion metrics for comparison with other algorithms
accuracy_logistic <- 0.7418
sensitivity_l <- 0.4313
precision_l <- 483/(483+204)
recall_l <- 483/(483+637)
#recall_l
f1_score_l <- 2*(precision_l*recall_l)/(precision_l + recall_l)
#f1_score_l


metrics_log <- data.frame(Metric = c("Accuracy", "F1 Score", "Recall", "Precision"),
                         Value = c(accuracy_logistic, f1_score_l, recall_l, precision_l))


```
 
```{r Table3 , results = 'asis'}

metrics_l <- xtable(metrics_log, caption = "Metrics for Logistic Regression \\label{tab1}")
  print.xtable(metrics_l,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )

```
 

```{r Table4 , results = 'asis'}


library(broom)
model3 <- glm(
  LeaveOrNot ~ Age + male + benched + ExperienceInCurrentDomain + bangalore + pune + education + PaymentTier + Y2014 + Y2015 + Y2016 + Y2017,
  family = "binomial",
  data = churn_train
  )

t <- tidy(model3)
#t

table_results <- xtable(t, caption = "Logistic Regression Results \\label{tab1}")
  print.xtable(table_results,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )
  


```


# KNN  {-}

The K-nearest neighbour (KNN) algorithm predicts each observation based on its similarity to other observations. It identifies "k" observations that are most similar .. and uses the most common class of those k observations as the predicted output ().

Using the KNN approach, a grid-search is conducted to find the optimal level of K. Low values of K makes the model sensitive to noise and less generalisable while large values can lead to oversmoothing.

The accuracy metric is used, given that is an appropriate metric for a classification problem. The grid search uses cross-validation techniques to looks for the optimal level of K between 2 and 25. The model selects k =3 as the optimal value. The accuracy rate for k=3 is 78.3%. For the testing data, the model's accuracy is slightly lower at 77%. 

Alongside the model's accuracy, the precision, recall and F1 score are also examined. Precision measures the proportion of correctly predicted positive instances, also called true positives, out of all instances predicted as positive. A high precision indicates few false positives. In this case the precision is 74%, which is relatively high.

Recall, also known as the model's sensitivity, measures the proportion of correctly predicted positive instances out of all actual positives instances. In other words, it is the proportion of true positives out of both the true positives and false negatives. A higher recall indicates few false negatives. In this instance, the recall is much lower (52%). This indicates that the model has a higher false negative rate than false positive rate.

The F1 score balances both the precision and recall, and provides an indication of the overall performance of the model. The F1 score for the optimal K-nearest neighbours model is 61.2%, indicating that the model is somewhat adequate.

```{r KNN,  warning =  FALSE, fig.align = 'center', fig.cap = "Caption Here \\label{Figure1}", fig.ext = 'png', fig.height = 3, fig.width = 6}


split <- rsample::initial_split(df, prop = 0.7, strata = "LeaveOrNot")
KNN_emp_train <- rsample::training(split)
KNN_emp_test <- rsample::testing(split)


cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Create grid of hyperparameter values
# hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
# knn_fit <- train(
#   LeaveOrNot ~ ., 
#   data = KNN_emp_train, 
#   method = "knn", 
#   trControl = cv, 
#   tuneGrid = hyper_grid,
#   metric = "Accuracy"
# )

knn_fit <- train(
  LeaveOrNot ~ ., 
  data = KNN_emp_train, 
  method = "knn", 
  trControl = cv, 
  tuneLength = 1,  # Use tuneLength = 1 to avoid running grid search
  metric = "Accuracy",
  tuneGrid = data.frame(k = 3)  # Set k = 3 directly
)

# knn_fit
# 
# plot(knn_fit)


KNN_predictions <- predict(knn_fit, newdata = KNN_emp_test)
# confusionMatrix(KNN_predictions, KNN_emp_test$LeaveOrNot)


#predictions
predicted_labels_KNN <- KNN_predictions
#predicted_labels

#accuracy
actual_labels_KNN <- KNN_emp_test$LeaveOrNot
accuracy_KNN <- sum(KNN_predictions == actual_labels_KNN) / length(actual_labels_KNN)
#accuracy_KNN

#precision
precision_KNN <- sum(predicted_labels_KNN == 1 & actual_labels_KNN == 1) / sum(predicted_labels_KNN == 1)
#precision_KNN

#recall
recall_KNN <- sum(predicted_labels_KNN == 1 & actual_labels_KNN == 1) / sum(actual_labels_KNN == 1)
#recall_KNN

#f1-score
f1_score_KNN <- 2 * (precision_KNN * recall_KNN) / (precision_KNN + recall_KNN)
#f1_score_KNN


metrics_knn <- data.frame(Metric = c("Accuracy", "F1 Score", "Recall", "Precision"),
                         Value = c(accuracy_KNN, f1_score_KNN, recall_KNN, precision_KNN))

```

```{r Table5 , results = 'asis'}

metrics_k <- xtable(metrics_knn, caption = "Metrics for KNN \\label{tab1}")
  print.xtable(metrics_k,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )


```


KNN models are very sensitive to feature scaling. Results can easily become biased as variables with larger scales can dominate the distance to calculation.

On the other hand, KNN is appropriate for small datasets such as this one. 

# Random Forests {-}

Random forests are powerful out-of the box algorithms that generally have very good predictive accuracy (). The come with the benefits of decision trees and bagging but greatly reduce instability and between-tree correlation.

## Decision tree {-}

Following the general rule-of-thumb, there is a 70:30 split among the training and testing data.

```{r}

split <- rsample::initial_split(df, prop = 0.7, strata = "LeaveOrNot")
emp_train <- rsample::training(split)
emp_test <- rsample::testing(split)

#first decision tree


emp_dt1 <- rpart(
  formula = LeaveOrNot ~ .,
  data = emp_train,
  method = "class"
)

rpart.plot(emp_dt1)


```

#ranger - model$confusionmatrix

```{r}

# Make predictions on the test set
predictions <- predict(emp_dt1, newdata = emp_test, type = "class")

actual_labels <- emp_test$LeaveOrNot

# Calculate the accuracy
accuracy <- sum(predictions == actual_labels) / length(actual_labels) * 100


# Print the accuracy
cat("Accuracy:", accuracy, "%\n")

##pruning - before pruning, variance is high and bias is low

cv_emp_dt1 <- prune(emp_dt1, cp = emp_dt1$cptable[which.min(emp_dt1$cptable[,"xerror"]),"CP"])
#
# plotcp(emp_dt1)
#
rpart.plot(cv_emp_dt1)


```


The results from the baseline random forest are presented below. Random Forests help to reduce tree correlation. It does so by using split-variable randomisation. In the baseline model, number of trees are set to 500 by default. It is possible to alter the m(try) parameter, but currently it is set to the square root of the number of parameters, given that this standard when doing a classification problem.

```{r Figure2, warning =  FALSE, fig.align = 'center', fig.cap = "Caption Here \\label{Figure2}", fig.height = 3, fig.width = 6, dev = 'png'}

library(ranger)
#number of features
n_features <- length(setdiff(names(emp_train), "LeaveOrNot"))

emp_train$LeaveOrNot <- as.factor(emp_train$LeaveOrNot)

emp_rf1 <- ranger(
  LeaveOrNot ~ .,
  data = emp_train,
  mtry = floor(sqrt(n_features)),
  respect.unordered.factors = "order",
  seed = 123)

#(default_rmse <- sqrt(emp_rf1$prediction.error))

#prediction
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
predictions_rf <- predict(emp_rf1, data = emp_test)
#predictions
predicted_labels_rf <- predictions_rf$predictions
#predicted_labels

#accuracy
actual_labels_rf <- emp_test$LeaveOrNot
accuracy_rf <- sum(predictions_rf$predictions == actual_labels_rf) / length(actual_labels_rf)
# accuracy_rf

#precision
precision_rf <- sum(predicted_labels_rf == 1 & actual_labels_rf == 1) / sum(predicted_labels_rf == 1)
# precision_rf

#recall
recall_rf <- sum(predicted_labels_rf == 1 & actual_labels_rf == 1) / sum(actual_labels_rf == 1)
# recall_rf

#f1-score
f1_score_rf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)
# f1_score_rf

library(pROC)
predicted_probs <- as.numeric(predictions_rf$predictions)

# Convert the actual labels to a numeric format (0 and 1)
numeric_labels_rf <- as.numeric(actual_labels_rf) - 1

# Calculate the ROC curve and AUC-ROC
roc_obj <- roc(numeric_labels_rf, predicted_probs)
auc_roc <- auc(roc_obj)
# auc_roc


metrics_df <- data.frame(Metric = c("Accuracy", "F1 Score", "Recall", "Precision", "AUC ROC"),
                         Value = c(accuracy_rf, f1_score_rf, recall_rf, precision_rf, auc_roc))
```


```{r Table6 , results = 'asis'}

#print(metrics_df)

metrics <- xtable(metrics_df, caption = "Metrics for Baseline Random Forest \\label{tab1}")
  print.xtable(metrics,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )
  
  
  
```


Comparing the training and testing error, the test error (15.1%) is slightly higher than the training error (11.8%). This may indicate that there is some level of overfitting, given that the training data performs better, however it does not appear to be substantial. The model's performance is still reasonably good. 

To continue to examine the bias-variance tradeoff, the learning curve is plotted. At small sample sizes, it can be seen that the test accuracy is much lower than the training dataset. The high accuracy for the training set at lower sample sizes indicates overfitting. Once the sample size reaches over 2000, the accuracy between the training and the testing set begin to converge, reducing the bias-variance tradeoff.

```{r}

train_predictions <- predict(emp_rf1, data = emp_train)$predictions

# Calculate training accuracy
train_accuracy <- sum(train_predictions == emp_train$LeaveOrNot) / nrow(emp_train)

# Make predictions on test set
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
test_predictions <- predict(emp_rf1, data = emp_test)$predictions

# Calculate test accuracy
test_accuracy <- sum(test_predictions == emp_test$LeaveOrNot) / nrow(emp_test)

# cat("Training Accuracy:", train_accuracy, "\n")
# cat("Test Accuracy:", test_accuracy, "\n")

# Calculate training and test errors
train_error <- 1 - train_accuracy
test_error <- 1 - test_accuracy

# Print the training and test errors
# cat("Training Error:", train_error, "\n")
# cat("Test Error:", test_error, "\n")

accuracy_df <- data.frame(Metric = c("Training Accuracy", "Test Accuracy", "Training Error", "Test Error"),
                         Value = c(train_accuracy, test_accuracy, train_error, test_error))

```

```{r Table7 , results = 'asis'}

accu <- xtable(accuracy_df, caption = "More Metrics for Baseline Random Forest \\label{tab1}")
  print.xtable(accu,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )
```

There are several hyper-parameters to consider in this model, including the number of trees, the number of features to consider at a given split, the complexity of each tree, the sampling scheme, and the splitting rule to use during tree construction.

The first parameter I adjust is the number of trees. If I have 15 variables, I will make 150 trees. The default above was 500 trees. 

Adjusting the number of trees down from 500 to 150 increases the accuracy of the model, but marginally. Accuracy increased from 84.81% to 84.96%.

Following the baseline random forest model, a grid search is conducted over a range of hyperparameters in an attempt to select the optimal model. 

The default sampling scheme for a random forest is one with replacement.

Sample size influences how many observations are drawn for the training of each tree. Decreasing the sample size leads to more diverse trees and less between-tree correlation, which has a positive effect on predictive accuracy. Having a few features that 

Having many categorical features with varying number of levels, such as experience or education in this case, or unbalanced categories, then sampling with replacement can lead to biased results. Sampling without replacement can thus lead to a less biased use of all the levels across the trees in the random forest.  

I included the number of trees in the search. As a rule of thumb, the number of trees is 100, 150 and 250 were selected as possibilities.

```{r}
# hyper_grid <- expand.grid(
#   mtry = c(3, floor(sqrt(n_features)), 4),
#   min.node.size = c(1, 3, 5, 10), 
#   replace = c(TRUE, FALSE),                               
#   sample.fraction = c(.5, .63, .8),
#   num.trees = c(100, 150, 250),
#   rmse = NA                                               
# )
# 
# # execute full cartesian grid search
# for(i in seq_len(nrow(hyper_grid))) {
#   # fit model for ith hyperparameter combination
#   fit <- ranger(
#     formula         = LeaveOrNot ~ ., 
#     data            = emp_train, 
#     num.trees       = hyper_grid$num.trees[i],
#     mtry            = hyper_grid$mtry[i],
#     min.node.size   = hyper_grid$min.node.size[i],
#     replace         = hyper_grid$replace[i],
#     sample.fraction = hyper_grid$sample.fraction[i],
#     verbose         = FALSE,
#     seed            = 123,
#     respect.unordered.factors = 'order',
#   )
#   # export OOB error 
#   hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
# }
# 
# # assess top 10 models
# hyper_grid %>%
#   arrange(rmse) %>%
#   mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
#   head(10)
```

The best model selected is one with m(try) set to 4, a number of trees as 250, a node size of 1, sample without replacement, and a sample fraction of 0.63. 

```{r}
emp_rf2 <- ranger(
  LeaveOrNot ~ .,
  data = emp_train,
  num.trees = 250,
  mtry = 4,
  min.node.size = 1,
  sample.fraction = .63,
  replace = FALSE,
  respect.unordered.factors = "order",
  seed = 123)

(final_rmse <- sqrt(emp_rf2$prediction.error))

#prediction
predictions_rf2 <- predict(emp_rf2, data = emp_test)
#predictions
predicted_labels_rf2 <- predictions_rf2$predictions
#predicted_labels

#accuracy
actual_labels_rf2 <- emp_test$LeaveOrNot
accuracy_rf2 <- sum(predictions_rf2$predictions == actual_labels_rf2) / length(actual_labels_rf2)
#accuracy_rf2

#precision
precision_rf2 <- sum(predicted_labels_rf2 == 1 & actual_labels_rf2 == 1) / sum(predicted_labels_rf2 == 1)
#precision_rf2

#recall
recall_rf2 <- sum(predicted_labels_rf2 == 1 & actual_labels_rf2 == 1) / sum(actual_labels_rf2 == 1)
#recall_rf2

#f1-score
f1_score_rf2 <- 2 * (precision_rf2 * recall_rf2) / (precision_rf2 + recall_rf2)
#f1_score_rf2

library(pROC)
predicted_probs2 <- as.numeric(predictions_rf$predictions)

# Convert the actual labels to a numeric format (0 and 1)
numeric_labels_rf2 <- as.numeric(actual_labels_rf2) - 1

# Calculate the ROC curve and AUC-ROC
roc_obj2 <- roc(numeric_labels_rf2, predicted_probs)
auc_roc2 <- auc(roc_obj)
#auc_roc2

```

```{r Table8 , results = 'asis'}

metrics_df2 <- data.frame(Metric = c("Accuracy", "F1 Score", "Recall", "Precision", "AUC ROC"),
                         Value = c(accuracy_rf2, f1_score_rf2, recall_rf2, precision_rf2, auc_roc2))

metrics2 <- xtable(metrics_df2, caption = "Metrics for Tuned Random Forest \\label{tab1}")
  print.xtable(metrics2,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )

```

```{r}

train_predictionsrf2 <- predict(emp_rf2, data = emp_train)$predictions

# Calculate training accuracy
train_accuracyrf2 <- sum(train_predictionsrf2 == emp_train$LeaveOrNot) / nrow(emp_train)

# Make predictions on test set
# emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
test_predictionsrf2 <- predict(emp_rf2, data = emp_test)$predictions

# Calculate test accuracy
test_accuracyrf2 <- sum(test_predictionsrf2 == emp_test$LeaveOrNot) / nrow(emp_test)

# cat("Training Accuracy:", train_accuracy, "\n")
# cat("Test Accuracy:", test_accuracy, "\n")

# Calculate training and test errors
train_errorrf2 <- 1 - train_accuracyrf2
test_errorrf2 <- 1 - test_accuracyrf2

# Print the training and test errors
# cat("Training Error:", train_error, "\n")
# cat("Test Error:", test_error, "\n")
```

```{r Table9 , results = 'asis'}

accuracyrf_df <- data.frame(Metric = c("Training Accuracy", "Test Accuracy", "Training Error", "Test Error"),
                         Value = c(train_accuracyrf2, test_accuracyrf2, train_errorrf2, test_errorrf2))

accurf <- xtable(accuracyrf_df, caption = "More Metrics for Tuned Random Forest \\label{tab1}")
  print.xtable(accurf,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom'
             )

```

Feature importance is considered.

```{r}
emp_train$Experience <- emp_train$ExperienceInCurrentDomain
emp_train$ExperienceInCurrentDomain <- NULL
emp_test$Experience <- emp_test$ExperienceInCurrentDomain
emp_test$ExperienceInCurrentDomain <- NULL

# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = LeaveOrNot ~ ., 
  data = emp_train, 
  num.trees = 250,
  mtry = 4,
  min.node.size = 1,
  sample.fraction = .63,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = LeaveOrNot ~ ., 
  data = emp_train, 
  num.trees = 250,
  mtry = 4,
  min.node.size = 1,
  sample.fraction = .63,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
p1 <- vip::vip(rf_impurity, num_features = 14, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 14, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)

```


<!-- :::::: {.columns data-latex="[T]"} -->
<!-- ::: {.column data-latex="{0.7\textwidth}"} -->
<!-- ```{r, echo=FALSE, fig.width=4, fig.height=4} -->
<!-- par(mar = c(4, 4, .2, .1)) -->
<!-- plot(cars, pch = 19) -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: {.column data-latex="{0.05\textwidth}"} -->
<!-- \ -->
<!-- ::: -->
<!-- ::: {.column data-latex="{0.2\textwidth}"} -->
<!-- \scriptsize -->

<!-- ## Data {-} -->
<!-- The figure on the left-hand side shows the `cars` data. -->

<!-- Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do -->
<!-- eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut -->
<!-- enim ad minim veniam, quis nostrud exercitation ullamco laboris -->
<!-- nisi ut aliquip ex ea commodo consequat. -->
<!-- ::: -->
<!-- :::::: -->







<!-- $$ -->
<!-- This is a commented out section in the writing part. -->
<!-- Comments are created by highlighting text, amnd pressing CTL+C -->
<!-- \\begin{align} -->
<!-- \\beta = \\alpha^2 -->
<!-- \end{align} -->
<!-- $$ -->



\hfill

<!-- hfill can be used to create a space, like here between text and table. -->



# Conclusion

In order to predict employee attrition, various models and their accuracy levels are assessed. 

The best model in predicting is the random forest model after hyperparameter tuning. This model achieves an accuracy rate of 86 percent.

To cite this package, simply type citation("Texevier") in Rstudio to get the citation for @Texevier (Note that uncited references in your bibtex file will not be included in References).

<!-- Make title of bibliography here: -->
<!-- \newpage -->

\newpage

# References {-}

<div id="refs"></div>




