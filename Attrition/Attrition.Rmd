---
title: "Predicting Employee Attrition"
author: "H MacGinty"
date: "23 June 2023"
# date: "`r Sys.Date()`"
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
output:
  pagedown::html_paged:
    # template: wp_paged.html
    # css: ['wp.css', 'wp-fonts.css', 'wp-page.css']
    css: ["Template/default-fonts-Texevier.css", "Template/default-page-Texevier.css", "Template/default-Texevier.css"]
    csl: Template/harvard-stellenbosch-university.csl # referencing format used.
    template: ["Template/paged-Texevier.html"]

    toc: true
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    self_contained: TRUE
abstract: |
    This paper investigates employee attributes and attrition. Random forests are used to build a model to predict employee attrition based on key attributes, such as education, pay, gender, and age, among others.

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
pacman::p_load(modelsummary, gt, knitr, kableExtra, tidyverse)
```

\newpage

# Introduction {-}

Losing employees can be costly for businesses. Predicting attrition and its key determinants or attributes can help estimate future employee turnover and attempt to reduce it. 

<!-- The following is a code chunk. It must have its own unique name (after the r), or no name. After the comma follows commands for R which are self-explanatory. By default, the code and messages will not be printed in your pdf, just the output: -->

```{r }
employee <- read.csv("C:/Users/hanna/OneDrive/Documents/Data Science/Project/EmployeeProject/Attrition/data/Employee.csv") 

library(wesanderson)
palette <- wes_palette("GrandBudapest1", 2)
palette2 <- wes_palette("GrandBudapest1", 1)



```



# Exploratory Data Analaysis {-}

The dataset contains information on 4653 employees and whether they attrited or not. The employee attributes available include demographic information such as age and gender. Employees are based in one of three major cities in India, namely Bangalore, Pune and New Delhi. The year an employee joined a company (Joining Year), ranging from 2014 to 2018, is also considered. 

Given that earnings are generally an important determinant in whether an employee leaves their job, their payment tier, scaled from 1, being the highest, and 3, being the lowest, is included in the data. Additionally, years of experience in their current field is included as well as their highest level of education (Bachelor's, Master's, PhD). There is also information on whether an employee kept out of projects for 1 month or more, which could potentially indicates an employee's lack of interest in work or plans to leave the company. 


## Graphical analysis

```{r}
#spread of data across city and education
pacman::p_load(forcats)

#graphing city spread among employees
# employee %>% 
# mutate(City2 = forcats::fct_infreq(City)) %>% 
# ggplot() + 
# geom_bar(aes(x = City2), fill = palette2, alpha = 0.7) + coord_flip() + 
# labs(x = "Count", y = "City", title = "City spread among employees", 
#     caption = "Data from Kaggle") + theme_classic()

source("code/plot_spread.R")
spread <- plot_spread(employee, City)
spread

#graph spread of education
# employee %>% 
# mutate(Education2 = forcats::fct_infreq(Education)) %>% 
# ggplot() + 
# geom_bar(aes(x = Education2), fill = palette2, alpha = 0.7) + coord_flip() + 
# labs(x = "Count", y = "Education", title = "Education spread among employees", 
#     caption = "Data from Kaggle") + theme_classic()

spread_ed <- plot_spread(employee, Education)
spread_ed
phd_count <- sum(employee$JoiningYear == "2017")
```

Spreading the data among city, most of the employees are from Bangalore. Additionally, most employees have a Bachelors degree. Only 179 employees have a PHD and 873 have Master's degrees. Looking at age, which ranges between 22 and 41, the majority of employees are in their mid to late twenties, skewing the distribution to right.  

Regarding experience, very few individuals have 6 or 7 years experience (only 16 employees in total) in their current field of work, most likely due to the young employee base. Most commonly, individuals have 2 years experience. 

2017 saw the most employees join the company. There was a substantial fall in employees joining in 2018. Only 367 employees joined in 2018, compared to 1180 in 2017.

```{r}
#distribution
ggplot(employee, aes(x = Age)) +
  geom_histogram(binwidth = 2, fill = palette2, color = "black") +
  labs(x = "Age", y = "Count")  +
  ggtitle("Distribution of Age") + theme_classic()


ggplot(employee, aes(x = ExperienceInCurrentDomain)) +
  geom_histogram(binwidth = 1, fill = palette2, color = "black") +
  labs(x = "Experience", y = "Count") +
  ggtitle("Distribution of Experience") +theme_classic()

ggplot(employee, aes(x = JoiningYear)) +
  geom_histogram(binwidth = 1, fill = palette2, color = "black") +
  labs(x = "Year Joined", y = "Count") +
  ggtitle("Distribution of Joining Year") +theme_classic()
```

Attrition rates are highest among those with Master's degrees. Nearly fifty percent of those with Master's degrees left their job. When looking across joining year, almost all the employees that joined in 2018 resigned. It is possible that some event occurred in 2018 that caused that cohort to leave within the next two years.  

```{r}
#Attrition Rate
ggplot(employee, aes(x = Education, fill = factor(LeaveOrNot))) +
  geom_bar(position = "fill") + scale_fill_manual(values = palette) +
  labs(x = "Education", y = "Attrition Rate", fill = "Attrition") +
  ggtitle("Attrition Rates by Education") + theme_classic()

ggplot(employee, aes(x = factor(JoiningYear), fill = factor(LeaveOrNot))) +
  geom_bar(position = "fill") +
  labs(x = "Joining Year", y = "Attrition Rate", fill = "Attrition") + scale_fill_manual(values = palette) +
  ggtitle("Attrition Rates by Joining Year") + theme_minimal()

#Looking at employee attrition against age. It seems that attrition is higher among younger individuals and lower for older individuals.

smooth <- ggplot(employee, aes(x = Age, y = LeaveOrNot)) +
  geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE, color = "red") +
  labs(x = "Age", y = "Employee Attrition") +
  theme_classic()

smooth


```


<!-- $$ -->
<!-- This is a commented out section in the writing part. -->
<!-- Comments are created by highlighting text, amnd pressing CTL+C -->
<!-- \\begin{align} -->
<!-- \\beta = \\alpha^2 -->
<!-- \end{align} -->
<!-- $$ -->

Table 1 below shows the attrition rate for each category. Attrition rates are particularly high for individuals who joined in 2018 (98%), matching the graphical analysis. It is also high for those earning a mid-tier salary (almost 60%). This is followed by 50% of those from Pune attriting.

```{r table-1, echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
library(knitr)
library(kableExtra)

#Table of attrition rate
attrition_rate_education <- employee %>%
  group_by(Education) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

# Calculate attrition rate by Gender
attrition_rate_gender <- employee %>%
  group_by(Gender) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by City
attrition_rate_city <- employee %>%
  group_by(City) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by Joining Year
attrition_rate_year <- employee %>%
  group_by(JoiningYear) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

#Calculate attrition rate by Payment Tier
attrition_rate_pay <- employee %>%
  group_by(PaymentTier) %>%
  summarize(Attrition_Rate = mean(LeaveOrNot) * 100)

education_labels <- c("Bachelors", "Masters", "PHD") 
city_labels <- c("Bangalore", "New Delhi", "Pune") 
gender_labels <- c("Female", "Male")  
year_labels <- c("2012", "2013", "2014", "2015", "2016", "2017", "2018")  
pay_labels <- c("1", "2", "3") 

education_table <- data.frame(Variable = education_labels, Attrition_Rate = attrition_rate_education$Attrition_Rate)
city_table <- data.frame(Variable = city_labels, Attrition_Rate = attrition_rate_city$Attrition_Rate)
gender_table <- data.frame(Variable =gender_labels, Attrition_Rate = attrition_rate_gender$Attrition_Rate)
year_table <- data.frame(Variable = year_labels, Attrition_Rate = attrition_rate_year$Attrition_Rate)
pay_table <- data.frame(Variable = pay_labels, Attrition_Rate = attrition_rate_pay$Attrition_Rate)

colnames(education_table) <- c("Variable", "Attrition Rate")
colnames(city_table) <- c("Variable", "Attrition Rate")
colnames(gender_table) <- c("Variable", "Attrition Rate")
colnames(year_table) <- c("Variable", "Attrition Rate")
colnames(pay_table) <- c("Variable", "Attrition Rate")

combined_table <- rbind(
  data.frame(Variable = "Education", Category = education_table$Variable, `Attrition Rate` = education_table$`Attrition Rate`),
  data.frame(Variable = "City", Category = city_table$Variable, `Attrition Rate` = city_table$`Attrition Rate`),
  data.frame(Variable = "Gender", Category = gender_table$Variable, `Attrition Rate` = gender_table$`Attrition Rate`),
  data.frame(Variable = "Year", Category = year_table$Variable, `Attrition Rate` = year_table$`Attrition Rate`),
  data.frame(Variable = "Pay", Category = pay_table$Variable, `Attrition Rate` = pay_table$`Attrition Rate`)
)


table <- kable(combined_table, row.names = TRUE,
      caption = 'Attrition Rate Table by Attribute',
      format = "html", booktabs = T) %>%
        kable_styling(full_width = T,
                      latex_options = c("striped",
                                        "scale_down",
                                        "HOLD_position"),
                      font_size = 13)
table
```

# Pre-processing {-}
## Feature and Target Engineering {-}

Since my predictor variable (Leave or Not) is binary, there is no need for target engineering.

Regarding feature engineering, most of the features are categorical. Gender and whether a person benched or not (removed themselves from projects in the lst month) are transformed into dummies. Joining year is one-hot encoded, resulting in binary variables for each of the 5 joining years. 

Education is label encoded as it can be ordered (Bachelors being the lowest level of education, Master's one higher and PHD being the highest level of education). Payment Tier and experience in the current domain were already label encoded and thus do not require further engineering. 

Since age is numeric and random forests are able to handle both numeric and categorical variables, it is not altered.

```{r}
#create a binary variable for gender - female is 0 and male is 1
employee <- employee %>%
  mutate(male = ifelse(Gender == "Female", 0, 1))

#create a binary variable for ever-benched - i.e., they kept out of projects for a month or more
employee <- employee %>%
  mutate(benched = ifelse(EverBenched == "No", 0, 1))

#One hot encoding
employee <- employee %>%
  mutate(bangalore = ifelse(City == "Bangalore", 1, 0)) %>% 
    mutate(pune = ifelse(City == "Pune", 1, 0)) %>% 
    mutate(new_delhi = ifelse(City == "New Delhi", 1, 0))

#lable encode education category where Bachelors =1, Master's =2 and PhD = 3
employee <- employee %>%
  mutate(education = case_when(
    Education == "Bachelors" ~ 1,
    Education == "Masters" ~ 2,
    Education == "PHD" ~ 3
  ))

#one-hot encode joining year
data <- data.frame(JoiningYear = c("2014", "2015", "2016", "2017", "2018"))

employee <- employee %>%
  mutate(Y2014 = ifelse(JoiningYear == "2014", 1, 0)) %>% 
    mutate(Y2015 = ifelse(JoiningYear == "2015", 1, 0)) %>% 
    mutate(Y2016 = ifelse(JoiningYear == "2016", 1, 0)) %>% 
    mutate(Y2017 = ifelse(JoiningYear == "2017", 1, 0)) %>% 
     mutate(Y2018 = ifelse(JoiningYear == "2018", 1, 0))



```


# Bias-Variance Trade-off {-}

It is also important to consider the bias-variance trade-off. Prediction errors are generally a result of either bias or variance. In general, decreasing bias will almost always lead to greater variance. Bias is the difference between the expected prediction and the correct value (linear models have high levels of bias).

Variance error is the variability of a model prediction for a given data point. Some models can potentially overfit the training data, resulting in accurate results against the training data but typically poor results against the test data. In other words, the model does not generalise well.

We can adjust the model hyperparameters to achieve the best mix of bias and variance. 






\newpage

# Multiple Logistic Regression {-}
```{r table-2, echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
library(caret)
library(vip)
library(rsample)
library(rpart)
library(rpart.plot)
library(broom)
library(vip)

employeedata <- subset(employee, select = c(LeaveOrNot, Age, male, benched, ExperienceInCurrentDomain, bangalore, pune, new_delhi, education, Y2014, Y2015, Y2016, Y2017, Y2018, PaymentTier))
# 
# 
# 
df <- employeedata %>% mutate_if(is.ordered, factor, ordered = FALSE)
df$LeaveOrNot <- factor(df$LeaveOrNot, levels = c(0, 1))
df$male <- factor(df$male, levels = c(0, 1))
df$benched <- factor(df$benched, levels = c(0, 1))
df$bangalore <- factor(df$bangalore, levels = c(0, 1))
df$pune <- factor(df$pune, levels = c(0, 1))
df$new_delhi <- factor(df$new_delhi, levels = c(0, 1))
# 
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = .7)
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)

# 
#Let's make various models for comparison
set.seed(123)
cv_model1 <- train(
  LeaveOrNot ~ education,
  data = churn_train,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

# 
set.seed(123)
cv_model2 <- train(
  LeaveOrNot ~ education + PaymentTier,
  data = churn_train,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)


set.seed(123)
cv_model3 <- train(
  LeaveOrNot ~ Age + male + benched + ExperienceInCurrentDomain + bangalore + pune + education + PaymentTier + Y2014 + Y2015 + Y2016 + Y2017,
  data = churn_train,
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

s <- summary(
  resamples(
    list(
      model1 = cv_model1,
      model2 = cv_model2,
      model3 = cv_model3
    )
  )
)$statistics$Accuracy

table_logistic <-  kable(s, row.names = TRUE,
      caption = 'Model Accuracy in Logistic Regressions',
      format = "html", booktabs = T) %>%
        kable_styling(full_width = T,
                      latex_options = c("striped",
                                        "scale_down",
                                        "HOLD_position"),
                      font_size = 13)
table_logistic
# 
# 
# pred_class <- predict(cv_model3, churn_train)
# 
# #create confusion matrix
# confusionMatrix(
#   data = relevel(pred_class, ref = "1"),
#   reference = relevel(churn_train$LeaveOrNot, ref = "1")
# )

#top influential variables
vip(cv_model3, num_features = 24)
```

From the logistic regression, model accuracy ranges from 71 and 74 percent for the full model. The weakest model is model 1, which regresses attrition on education, has an accuracy rate between 61 and 65 percent.

In terms of the most important features, gender and the joining year of 2017 are the most important predictive features. Most of the other variables do carry some level of imprtance, therefore the model is not overly reliant on gender and joining year. 


```{r table-3, echo=FALSE, message=FALSE, warning=FALSE, results="asis"}
# 
library(ROCR)

# Compute predicted probabilities
m1_prob <- predict(cv_model1, churn_train, type = "prob")$"1"
m3_prob <- predict(cv_model3, churn_train, type = "prob")$"1"
#
# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$LeaveOrNot) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m3_prob, churn_train$LeaveOrNot) %>%
  performance(measure = "tpr", x.measure = "fpr")

# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)
plot(perf2, add = TRUE, col = "blue")
legend(0.6, 0.3, legend = c("cv_model1", "cv_model3"),
       col = c("black", "blue"), lty = 2:1, cex = 0.6)

# #above is the ROC curve
# 
# #the below looks at PLS - but this may be inappropriate for data I have
# # set.seed(123)
# # cv_model_pls <- train(
# #   LeaveOrNot ~ .,
# #   data = churn_train,
# #   method = "pls",
# #   family = "binomial",
# #   trControl = trainControl(method = "cv", number = 10),
# #   preProcess = c("zv", "center", "scale"),
# #   tuneLength = 10
# # )
# # 
# # cv_model_pls$bestTune
# # 
# # cv_model_pls$results %>%
# #   dplyr::filter(ncomp == pull(cv_model_pls$bestTune))
# # 
# # ggplot(cv_model_pls)
# 
# class(employee$male)
# 
# #results from logistic regression
model3 <- glm(
  LeaveOrNot ~ Age + male + benched + ExperienceInCurrentDomain + bangalore + pune + education + PaymentTier + Y2014 + Y2015 + Y2016 + Y2017,
  family = "binomial",
  data = churn_train
  )

# coefficients <- coef(model3)
# std_errors <- summary(model3)$coefficients[, "Std. Error"]
# p_values <- summary(model3)$coefficients[, "Pr(>|z|)"]

# Combine coefficients, standard errors, and p-values into a data frame
# results_df <- data.frame(Coefficient = coefficients, "Std. Error" = std_errors, "p-value" = p_values)
# 
# # Format the data frame as a table
# results_table <- knitr::kable(results_df, caption = "Regression Results", align = "c")
# 
# # Display the results table
# results_table

# logsitic_results <- kable(results_df, row.names = TRUE,
#       caption = 'Model Accuracy in Logistic Regressions',
#       format = "html", booktabs = T) %>%
#         kable_styling(full_width = T,
#                       latex_options = c("striped",
#                                         "scale_down",
#                                         "HOLD_position"),
#                       font_size = 13)
# 
# logsitic_results

# # Get the model summary
# summary_table <- summary(model3)
# # 
# 
# sum(is.na(coefficients))
# sum(is.na(std_errors))
# sum(is.na(p_values))
# 
# regression_table <- tbl_regression(model3)
# 
# # Display the summary table
# regression_table

# # Extract coefficients and standard errors
# coefficients <- coef(model3)
# std_errors <- summary(model3)$coefficients[, "Std. Error"]
# p_values <- summary(model3)$coefficients[, "Pr(>|z|)"]
# 
# # Combine coefficients, standard errors, and p-values into a data frame
# results_df <- data.frame(Coefficient = coefficients, "Std. Error" = std_errors, "p-value" = p_values)
# 
# # Format the data frame as a table
# results_table <- knitr::kable(results_df, caption = "Regression Results", align = "c")
# 
# # Display the results table
# results_table


t <- tidy(model3)

logsitic_results <- kable(t, row.names = TRUE,
      caption = 'Model Accuracy in Logistic Regressions',
      format = "html", booktabs = T) %>%
        kable_styling(full_width = T,
                      latex_options = c("striped",
                                        "scale_down",
                                        "HOLD_position"),
                      font_size = 13)

logsitic_results

```

# KNN

looking at a K-Nearest Neighbours approach, a grid-search is conducted to find the optimal level of K. The accuracy metric is used, given that is an appropriate metric for a classification problem. The grid search looks for the optimal level of K between 2 and 25. The model selects k =5 as the optimal value. The accuracy rate for k=5 is 78.04%. 

Strangely, on the testing data, the model's accuracy is slightly higher at 79%, indicating a potential issue with the model such as data leakage.

```{r}

split <- rsample::initial_split(employeedata, prop = 0.7, strata = "LeaveOrNot")
emp_train <- rsample::training(split)
emp_test <- rsample::testing(split)
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)

# Convert the "LeaveOrNot" variable to a factor in the training set
emp_train$LeaveOrNot <- as.factor(emp_train$LeaveOrNot)
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
emp_test$male <- as.character(emp_test$male)
emp_test$benched <- as.character(emp_test$benched)
emp_test$bangalore <- as.character(emp_test$bangalore)
emp_test$pune <- as.character(emp_test$pune)
emp_test$new_delhi <- as.character(emp_test$new_delhi)
emp_test$education <- as.character(emp_test$education)
emp_test$Y2014 <- as.character(emp_test$Y2014)
emp_test$Y2015 <- as.character(emp_test$Y2015)
emp_test$Y2016 <- as.character(emp_test$Y2016)
emp_test$Y2017 <- as.character(emp_test$Y2017)
emp_test$Y2018 <- as.character(emp_test$Y2018)
emp_test$PaymentTier <- as.character(emp_test$PaymentTier)


#all characters in emp train and emp test except LeaveOrNot is a factor
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit <- train(
  LeaveOrNot ~ ., 
  data = emp_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "Accuracy"
)

knn_fit

plot(knn_fit)


#apply the model to the testing data

predictions <- predict(knn_fit, newdata = emp_test)
confusionMatrix(predictions, emp_test$LeaveOrNot)


#predictions
predicted_labels_KNN <- predictions
#predicted_labels

#accuracy
actual_labels_KNN <- emp_test$LeaveOrNot
accuracy_KNN <- sum(predictions == actual_labels_KNN) / length(actual_labels_KNN)
accuracy_KNN



```


# Random Forests {-}

Random forests are powerful out-of the box algorithms that generally have very good predictive accuracy (). The come with the benefits of decision trees and bagging but greatly reduce instability and between-tree correlation.

## Decision tree {-}

Following the general rule-of-thumb, there is a 70:30 split among the training and testing data.

```{r}

#first decision tree


emp_dt1 <- rpart(
  formula = LeaveOrNot ~ .,
  data = emp_train,
  method = "class"
)

rpart.plot(emp_dt1)



# Make predictions on the test set
predictions <- predict(emp_dt1, newdata = emp_test, type = "class")

actual_labels <- emp_test$LeaveOrNot

# Calculate the accuracy
accuracy <- sum(predictions == actual_labels) / length(actual_labels) * 100


# Print the accuracy
cat("Accuracy:", accuracy, "%\n")

##pruning - before pruning, variance is high and bias is low

cv_emp_dt1 <- prune(emp_dt1, cp = emp_dt1$cptable[which.min(emp_dt1$cptable[,"xerror"]),"CP"])
#
plotcp(emp_dt1)
#
rpart.plot(cv_emp_dt1)
#text(cv_emp_dt1)
#
cv_emp_dt1


```


The issue with bagging is that the trees in bagging are not independent (tree correlation). Random Forests help to reduce tree correlation. It does so by using split-variable randomisation. Number of trees should be 10 times number of features, as a rule of thumb. 

```{r}
library(ranger)
#number of features
n_features <- length(setdiff(names(emp_train), "LeaveOrNot"))

emp_train$LeaveOrNot <- as.factor(emp_train$LeaveOrNot)

emp_rf1 <- ranger(
  LeaveOrNot ~ .,
  data = emp_train,
  mtry = floor(sqrt(n_features)),
  respect.unordered.factors = "order",
  seed = 123)

(default_rmse <- sqrt(emp_rf1$prediction.error))

#prediction
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
predictions <- predict(emp_rf1, data = emp_test)
#predictions
predicted_labels <- predictions$predictions
#predicted_labels

#accuracy
actual_labels <- emp_test$LeaveOrNot
accuracy <- sum(predictions$predictions == actual_labels) / length(actual_labels)
accuracy

#precision
precision <- sum(predicted_labels == 1 & actual_labels == 1) / sum(predicted_labels == 1)
precision

#recall
recall <- sum(predicted_labels == 1 & actual_labels == 1) / sum(actual_labels == 1)
recall

#f1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score


library(pROC)
predicted_probs <- as.numeric(predictions$predictions)

# Convert the actual labels to a numeric format (0 and 1)
numeric_labels <- as.numeric(actual_labels) - 1

# Calculate the ROC curve and AUC-ROC
roc_obj <- roc(numeric_labels, predicted_probs)
auc_roc <- auc(roc_obj)
auc_roc


####

train_predictions <- predict(emp_rf1, data = emp_train)$predictions

# Calculate training accuracy
train_accuracy <- sum(train_predictions == emp_train$LeaveOrNot) / nrow(emp_train)

# Make predictions on test set
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
test_predictions <- predict(emp_rf1, data = emp_test)$predictions

# Calculate test accuracy
test_accuracy <- sum(test_predictions == emp_test$LeaveOrNot) / nrow(emp_test)

cat("Training Accuracy:", train_accuracy, "\n")
cat("Test Accuracy:", test_accuracy, "\n")

# Calculate training and test errors
train_error <- 1 - train_accuracy
test_error <- 1 - test_accuracy

# Print the training and test errors
cat("Training Error:", train_error, "\n")
cat("Test Error:", test_error, "\n")

```

Comparing the training and testing error, the test error (15.1%) is slightly higher than the training error (11.8%). This may indicate that there is some level of overfitting, given that the training data performs better, however it does not appear to be substantial. The model's performance is still reasonably good. 

To continue to examine the bias-variance tradeoff, the learning curve is plotted. At small sample sizes, it can be seen that the test accuracy is much lower than the training dataset. The high accuracy for the training set at lower sample sizes indicates overfitting. Once the sample size reaches over 2000, the accuracy between the training and the testing set begin to converge, reducing the bias-variance tradeoff.

```{r}
# Set up sample sizes
sample_sizes <- seq(100, nrow(emp_train), by = 100)

# Initialize empty vectors for accuracy values
train_accuracy <- numeric(length(sample_sizes))
test_accuracy <- numeric(length(sample_sizes))

# Train and evaluate models for different sample sizes
for (i in seq_along(sample_sizes)) {
  # Subset the training data
  subset_train <- emp_train[1:sample_sizes[i], ]
  
  # Train a random forest model
  rf_model <- ranger(
    LeaveOrNot ~ .,
    data = subset_train,
    mtry = floor(sqrt(n_features)),
    respect.unordered.factors = "order",
    seed = 123
  )
  
  # Make predictions on training data
  train_predictions <- predict(rf_model, data = subset_train)$predictions
  
  # Make predictions on test data
  test_predictions <- predict(rf_model, data = emp_test)$predictions
  
  # Calculate and store accuracies
  train_accuracy[i] <- sum(train_predictions == subset_train$LeaveOrNot) / nrow(subset_train)
  test_accuracy[i] <- sum(test_predictions == emp_test$LeaveOrNot) / nrow(emp_test)
}

# Plot the learning curve
plot(
  sample_sizes, train_accuracy,
  type = "l", col = "blue",
  xlab = "Sample Size", ylab = "Accuracy",
  ylim = c(0, 1), main = "Learning Curve"
)
lines(sample_sizes, test_accuracy, type = "l", col = "red")
legend("bottomright", legend = c("Train Accuracy", "Test Accuracy"),
       col = c("blue", "red"), lty = 1)


```


There are several hyperparameters to consider in this model, including the number of trees, the number of features to consider at a given split, the complexity of each tree, the sampling scheme, and the splitting rule to use during tree construction.

Node size is the most common method to control tree complexity.

The first parameter I am going to adjust is the number of trees. If I have 15 variables, I will make 150 trees. The default above was 500 trees. 

Adjusting the number of trees down from 500 to 150 increases the accuracy of the model, but marginally. Accuracy increased from 84.81% to 84.96%. 

```{r}
emp_train$LeaveOrNot <- as.factor(emp_train$LeaveOrNot)

emp_rf2 <- ranger(
  LeaveOrNot ~ ., 
  data = emp_train,
  mtry = floor(sqrt(n_features)),
  num.trees = 150, 
  respect.unordered.factors = "order",
  seed = 123)

(default_rmse <- sqrt(emp_rf2$prediction.error))


#prediction
emp_test$LeaveOrNot <- as.factor(emp_test$LeaveOrNot)
predictions <- predict(emp_rf2, data = emp_test)
#predictions
predicted_labels <- predictions$predictions
#predicted_labels

#accuracy
actual_labels <- emp_test$LeaveOrNot
accuracy <- sum(predictions$predictions == actual_labels) / length(actual_labels)
accuracy


```

It is possible to alter the m(try) parameter, but currently it is set to the square root of the number of parameters, given that this is a classification problem.

Node size is a common method to adjust tree complexity. 

```{r}

```


#Grid Search

Given that there are multiple hyperparameters and possible combinations to consider, a grid search is conducted to find the optimal model. 

```{r}
library(ranger)

# Define the hyperparameter grid
hyperparams <- list(
  num.trees = c(100, 150, 300),  # Example values, modify as needed
  mtry = c(floor(sqrt(n_features)), floor(n_features/3))  # Example values, modify as needed
)

# Create parameter combinations
param_combinations <- expand.grid(hyperparams)

# Split the data into training and validation sets
set.seed(123)
split <- rsample::initial_split(emp_train, prop = 0.7, strata = "LeaveOrNot")
train_data <- rsample::training(split)
validation_data <- rsample::testing(split)

# Define the evaluation metric
eval_metric <- function(true, predicted) {
  true <- as.numeric(as.character(true))
  predicted <- as.numeric(as.character(predicted))
  sqrt(mean((true - predicted)^2))
}

# Perform grid search
best_rmse <- Inf
best_model <- NULL

for (i in 1:nrow(param_combinations)) {
  params <- param_combinations[i, ]
  
  # Train the random forest model
  rf_model <- ranger(
    LeaveOrNot ~ .,
    data = train_data,
    num.trees = params$num.trees,
    mtry = params$mtry,
    seed = 123
  )
  
  # Make predictions on the validation set
  predictions <- predict(rf_model, data = validation_data)$predictions
  
  # Calculate the RMSE
  rmse <- eval_metric(validation_data$LeaveOrNot, predictions)
  
  # Check if this model is the best so far
  if (rmse < best_rmse) {
    best_rmse <- rmse
    best_model <- rf_model
  }
}

# Print the best model and its RMSE
print(best_model)
print(best_rmse)


```

The default sampling scheme for a random forest is one with replacement.

Sample size influences how many observations are drawn for the training of each tree. Decreasing the sample size leads to more diverse trees and less between-tree correlation, which has a positive effect on predictive accuracy. Having a few features that 

Having many categorical features with varying number of levels, such as experience or education in this case, or unbalanced categories, then sampling with replacement can lead to biased results. Sampling without replacement can thus lead to a less biased use of all the levels across the trees in the random forest.  

```{r}
hyper_grid <- expand.grid(
  mtry = c(3, floor(sqrt(n_features)), 4),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)

# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = LeaveOrNot ~ ., 
    data            = emp_train, 
    num.trees       = n_features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}

# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)
```


Assessing variable importance gives an indication of the variance in the model. If the model relies heavily on a few features, then this indicates high variance and over-fitting.  Comparing the two graphs below, it can be seen that with impurity-based variable importance, the model heavily relies on the joining year, specifically, those who joined the company in 2018. This is most likely due to the high attrition rate seen in the exploratory data analysis. Under permutation based variable importance, there is grater emphasis on more features such as education, payment tier and gender, despite joining year being the most important predictive feature.

```{r}
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = LeaveOrNot ~ ., 
  data = emp_train, 
  num.trees = 150,
  mtry = floor(sqrt(n_features)),
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = LeaveOrNot ~ ., 
  data = emp_train, 
  num.trees = 150,
  mtry = floor(sqrt(n_features)),
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
p1 <- vip::vip(rf_impurity, num_features = 14, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 14, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

# Gradient Boosting

```{r}

gbmdata <- employeedata

# Convert the "LeaveOrNot" variable to a factor
gbmdata$LeaveOrNot <- factor(gbmdata$LeaveOrNot)
gbmdata$male <- as.factor(gbmdata$male)
gbmdata$benched <- as.factor(gbmdata$benched)
gbmdata$bangalore <- as.factor(gbmdata$bangalore)
gbmdata$pune <- as.factor(gbmdata$pune)
gbmdata$new_delhi <- as.factor(gbmdata$new_delhi)
gbmdata$education <- as.factor(gbmdata$education)
gbmdata$Y2014 <- as.factor(gbmdata$Y2014)
gbmdata$Y2015 <- as.factor(gbmdata$Y2015)
gbmdata$Y2016 <- as.factor(gbmdata$Y2016)
gbmdata$Y2017 <- as.factor(gbmdata$Y2017)
gbmdata$Y2018 <- as.factor(gbmdata$Y2018)
gbmdata$PaymentTier <- as.factor(gbmdata$PaymentTier)

split <- rsample::initial_split(gbmdata, prop = 0.7, strata = "LeaveOrNot")
gbm_train <- rsample::training(split)
gbm_test <- rsample::testing(split)

set.seed(123)  # for reproducibility
emp_gbm1 <- gbm(
  formula = LeaveOrNot ~ .,
  data = gbm_train,
  distribution = "gaussian",  # SSE loss function
  n.trees = 5000,
  shrinkage = 0.1,
  interaction.depth = 3,
  n.minobsinnode = 10,
  cv.folds = 10
)

# find index for number trees with minimum CV error
best <- which.min(emp_gbm1$cv.error)

# get MSE and compute RMSE
sqrt(emp_gbm1$cv.error[best])

gbm.perf(emp_gbm1, method = "cv")


predictions <- factor(ifelse(predictions > 0.5, 1, 0),
                      levels = c(0, 1))

confusionMatrix(data = predictions, reference = gbm_test$LeaveOrNot)

```


# References {-}

<div id="refs"></div>


# Appendix {-}

## Appendix A {-}

Some appendix information here





